{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ee327f",
   "metadata": {},
   "source": [
    "# Protein Abundance Data Analysis with Embeddings\n",
    "\n",
    "This notebook processes protein abundance data and merges it with PPI network and sequence embeddings from STRING database for downstream machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad1336d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for: H.sapiens (ID: 9606)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "ORGANISM_CONFIG = {\n",
    "    'human': {'id': 9606, 'name': 'H.sapiens'},\n",
    "    'mouse': {'id': 10090, 'name': 'M.musculus'}\n",
    "}\n",
    "\n",
    "# Set target organism (change this to switch between human/mouse)\n",
    "TARGET_ORGANISM = 'human'  # Options: 'human', 'mouse'\n",
    "\n",
    "print(f\"Processing data for: {ORGANISM_CONFIG[TARGET_ORGANISM]['name']} (ID: {ORGANISM_CONFIG[TARGET_ORGANISM]['id']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971fe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein abundance data from: ../all_organisms_filtered_without_M.musculus_KIDNEY.parquet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Data file not found: ../all_organisms_filtered_without_M.musculus_KIDNEY.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load main dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_protein_abundance_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mload_protein_abundance_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading protein abundance data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(file_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Data file not found: ../all_organisms_filtered_without_M.musculus_KIDNEY.parquet"
     ]
    }
   ],
   "source": [
    "# Setup paths using relative paths from project root\n",
    "project_root = Path('../data')\n",
    "\n",
    "# Load original data\n",
    "#data_file = project_root / 'all_organisms_filtered_without_M.musculus_KIDNEY.parquet'\n",
    "\n",
    "#def load_protein_abundance_data(file_path):\n",
    "    #\"\"\"Load and validate protein abundance data.\"\"\"\n",
    "    #print(f\"Loading protein abundance data from: {file_path}\")\n",
    "    \n",
    "    #if not file_path.exists():\n",
    "        #raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    \n",
    "    #df = pd.read_parquet(file_path)\n",
    "    #print(f\"Loaded {len(df):,} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    #return df\n",
    "\n",
    "# Load main dataset\n",
    "#df = load_protein_abundance_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_organism_data(df, organism_key):\n",
    "    \"\"\"Filter dataframe for specific organism.\"\"\"\n",
    "    organism_id = ORGANISM_CONFIG[organism_key]['id']\n",
    "    organism_name = ORGANISM_CONFIG[organism_key]['name']\n",
    "    \n",
    "    print(f\"Filtering data for {organism_name} (ID: {organism_id})\")\n",
    "    organism_df = df[df['organism_id'] == organism_id].copy()\n",
    "    \n",
    "    print(f\"Found {len(organism_df):,} rows for {organism_name}\")\n",
    "    print(f\"Unique proteins: {organism_df['string_external_id'].nunique():,}\")\n",
    "    print(f\"Unique tissues: {organism_df['sample_organ'].nunique()}\")\n",
    "    \n",
    "    return organism_df\n",
    "\n",
    "# Filter for target organism\n",
    "organism_df = filter_organism_data(df, TARGET_ORGANISM)\n",
    "organism_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f725a",
   "metadata": {},
   "source": [
    "# Load Protein Embeddings\n",
    "\n",
    "Load both PPI network embeddings and sequence embeddings from STRING database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a545c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Embedding file not found: ../9606.protein.network.embeddings.v12.0.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Load PPI network embeddings\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m ppi_embeddings, ppi_proteins, ppi_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mload_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTARGET_ORGANISM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetwork\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mload_embeddings\u001b[0;34m(organism_key, embedding_type)\u001b[0m\n\u001b[1;32m     15\u001b[0m filename \u001b[38;5;241m=\u001b[39m project_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganism_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.protein.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.embeddings.v12.0.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Embedding file not found: ../9606.protein.network.embeddings.v12.0.h5"
     ]
    }
   ],
   "source": [
    "def load_embeddings(organism_key, embedding_type='network'):\n",
    "    \"\"\"\n",
    "    Load protein embeddings from HDF5 files.\n",
    "    \n",
    "    Args:\n",
    "        organism_key: 'human' or 'mouse'\n",
    "        embedding_type: 'network' or 'sequence'\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (embeddings_array, protein_list, metadata_dict)\n",
    "    \"\"\"\n",
    "    organism_id = ORGANISM_CONFIG[organism_key]['id']\n",
    "    \n",
    "    # Construct filename\n",
    "    filename = project_root / f\"{organism_id}.protein.{embedding_type}.embeddings.v12.0.h5\"\n",
    "    \n",
    "    if not filename.exists():\n",
    "        raise FileNotFoundError(f\"Embedding file not found: {filename}\")\n",
    "    \n",
    "    print(f\"Loading {embedding_type} embeddings from: {filename.name}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            # Load metadata\n",
    "            metadata = {}\n",
    "            for key in f['metadata'].attrs.keys():\n",
    "                metadata[key] = f['metadata'].attrs[key]\n",
    "                print(f\"{key}: {metadata[key]}\")\n",
    "            \n",
    "            # Load embeddings and protein names\n",
    "            embeddings = f['embeddings'][:]\n",
    "            proteins = [p.decode('utf-8') for p in f['proteins'][:]]\n",
    "            \n",
    "        print(f\"Successfully loaded {len(proteins):,} {embedding_type} embeddings\")\n",
    "        return embeddings, proteins, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {embedding_type} embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load PPI network embeddings\n",
    "ppi_embeddings, ppi_proteins, ppi_metadata = load_embeddings(TARGET_ORGANISM, 'network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9f94c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequence embeddings from: 9606.protein.sequence.embeddings.v12.0.h5\n",
      "embedding_dim: 1024\n",
      "n_proteins: 19699\n",
      "precision: 16\n",
      "Successfully loaded 19,699 sequence embeddings\n",
      "\n",
      "Protein lists match between PPI and sequence embeddings: True\n"
     ]
    }
   ],
   "source": [
    "# Load sequence embeddings\n",
    "sequence_embeddings, sequence_proteins, sequence_metadata = load_embeddings(TARGET_ORGANISM, 'sequence')\n",
    "\n",
    "# Verify protein lists match between embedding types\n",
    "proteins_match = set(ppi_proteins) == set(sequence_proteins)\n",
    "print(f\"\\nProtein lists match between PPI and sequence embeddings: {proteins_match}\")\n",
    "\n",
    "if not proteins_match:\n",
    "    ppi_set = set(ppi_proteins)\n",
    "    seq_set = set(sequence_proteins)\n",
    "    print(f\"PPI only: {len(ppi_set - seq_set)} proteins\")\n",
    "    print(f\"Sequence only: {len(seq_set - ppi_set)} proteins\")\n",
    "    print(f\"Common proteins: {len(ppi_set & seq_set)} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_dataframes(embeddings, proteins, embedding_type):\n",
    "    \"\"\"Convert embeddings to DataFrame format.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'string_external_id': proteins,\n",
    "        f'{embedding_type}_embeddings': list(embeddings)\n",
    "    })\n",
    "\n",
    "def merge_with_embeddings(abundance_df, ppi_embeddings, ppi_proteins, seq_embeddings, seq_proteins):\n",
    "    \"\"\"\n",
    "    Merge abundance data with both PPI and sequence embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with embeddings\n",
    "        dict: Merge statistics\n",
    "    \"\"\"\n",
    "    print(\"Creating embedding dataframes...\")\n",
    "    \n",
    "    # Create embedding dataframes\n",
    "    ppi_df = create_embedding_dataframes(ppi_embeddings, ppi_proteins, 'PPI')\n",
    "    seq_df = create_embedding_dataframes(seq_embeddings, seq_proteins, 'sequence')\n",
    "    \n",
    "    print(\"Merging abundance data with embeddings...\")\n",
    "    \n",
    "    # Merge with PPI embeddings\n",
    "    merged_ppi = abundance_df.merge(ppi_df, on='string_external_id', how='inner')\n",
    "    print(f\"After PPI merge: {len(merged_ppi):,} rows ({len(merged_ppi)/len(abundance_df)*100:.1f}% of original)\")\n",
    "    \n",
    "    # Merge with sequence embeddings\n",
    "    final_df = merged_ppi.merge(seq_df, on='string_external_id', how='inner')\n",
    "    print(f\"After sequence merge: {len(final_df):,} rows ({len(final_df)/len(abundance_df)*100:.1f}% of original)\")\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'original_rows': len(abundance_df),\n",
    "        'original_proteins': abundance_df['string_external_id'].nunique(),\n",
    "        'final_rows': len(final_df),\n",
    "        'final_proteins': final_df['string_external_id'].nunique(),\n",
    "        'merge_success_rate': len(final_df) / len(abundance_df),\n",
    "        'protein_coverage': final_df['string_external_id'].nunique() / abundance_df['string_external_id'].nunique()\n",
    "    }\n",
    "    \n",
    "    return final_df, stats\n",
    "\n",
    "# Perform the merge\n",
    "final_merged_df, merge_stats = merge_with_embeddings(\n",
    "    organism_df, ppi_embeddings, ppi_proteins, sequence_embeddings, sequence_proteins\n",
    ")\n",
    "\n",
    "# Display merge statistics\n",
    "print(f\"\\n=== Merge Statistics ===\")\n",
    "print(f\"Original proteins: {merge_stats['original_proteins']:,}\")\n",
    "print(f\"Final proteins: {merge_stats['final_proteins']:,}\")\n",
    "print(f\"Protein coverage: {merge_stats['protein_coverage']:.1%}\")\n",
    "print(f\"Row merge success: {merge_stats['merge_success_rate']:.1%}\")\n",
    "\n",
    "final_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"=== Data Quality Summary ===\")\n",
    "print(f\"Final dataset shape: {final_merged_df.shape}\")\n",
    "print(f\"Memory usage: {final_merged_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_cols = final_merged_df.isnull().sum()\n",
    "missing_cols = missing_cols[missing_cols > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nColumns with missing values:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        print(f\"  {col}: {count:,} ({count/len(final_merged_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found in merged dataset\")\n",
    "\n",
    "# Display sample of the data\n",
    "print(f\"\\nSample data preview:\")\n",
    "final_merged_df.sample(3)[['organism_name', 'sample_organ', 'abundance', 'string_external_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(df, organism_key):\n",
    "    \"\"\"Save the processed merged dataframe.\"\"\"\n",
    "    organism_name = ORGANISM_CONFIG[organism_key]['name'].replace('.', '_').lower()\n",
    "    output_path = project_root / f\"{organism_name}_abundance_PPI_seq_embeddings.parquet\"\n",
    "    \n",
    "    print(f\"Saving processed data to: {output_path}\")\n",
    "    \n",
    "    # Optimize memory before saving\n",
    "    print(f\"Pre-save memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Save with compression\n",
    "    df.to_parquet(output_path, engine='pyarrow', compression='snappy')\n",
    "    \n",
    "    file_size = output_path.stat().st_size / 1024**2\n",
    "    print(f\"Saved successfully! File size: {file_size:.1f} MB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Clean up large variables before saving to free memory\n",
    "del ppi_embeddings, sequence_embeddings\n",
    "del ppi_proteins, sequence_proteins\n",
    "\n",
    "# Save the processed data\n",
    "output_file = save_processed_data(final_merged_df, TARGET_ORGANISM)\n",
    "\n",
    "print(f\"\\n=== Processing Complete ===\")\n",
    "print(f\"Target organism: {ORGANISM_CONFIG[TARGET_ORGANISM]['name']}\")\n",
    "print(f\"Final dataset: {final_merged_df.shape[0]:,} rows, {final_merged_df.shape[1]} columns\")\n",
    "print(f\"Output file: {output_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
