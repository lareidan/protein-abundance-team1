{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ee327f",
   "metadata": {},
   "source": "# Protein Abundance Data Analysis with Embeddings\n\nThis notebook processes protein abundance data and merges it with PPI network and sequence embeddings from STRING database for downstream machine learning analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1336d",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport h5py\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nORGANISM_CONFIG = {\n    'human': {'id': 9606, 'name': 'H.sapiens'},\n    'mouse': {'id': 10090, 'name': 'M.musculus'}\n}\n\n# Set target organism (change this to switch between human/mouse)\nTARGET_ORGANISM = 'human'  # Options: 'human', 'mouse'\n\nprint(f\"Processing data for: {ORGANISM_CONFIG[TARGET_ORGANISM]['name']} (ID: {ORGANISM_CONFIG[TARGET_ORGANISM]['id']})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971fe66",
   "metadata": {},
   "outputs": [],
   "source": "# Setup paths using relative paths from project root\nproject_root = Path('..')\ndata_file = project_root / 'all_organisms_filtered_without_M.musculus_KIDNEY.parquet'\n\ndef load_protein_abundance_data(file_path):\n    \"\"\"Load and validate protein abundance data.\"\"\"\n    print(f\"Loading protein abundance data from: {file_path}\")\n    \n    if not file_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n    \n    df = pd.read_parquet(file_path)\n    print(f\"Loaded {len(df):,} rows and {len(df.columns)} columns\")\n    \n    return df\n\n# Load main dataset\ndf = load_protein_abundance_data(data_file)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b032e",
   "metadata": {},
   "outputs": [],
   "source": "def filter_organism_data(df, organism_key):\n    \"\"\"Filter dataframe for specific organism.\"\"\"\n    organism_id = ORGANISM_CONFIG[organism_key]['id']\n    organism_name = ORGANISM_CONFIG[organism_key]['name']\n    \n    print(f\"Filtering data for {organism_name} (ID: {organism_id})\")\n    organism_df = df[df['organism_id'] == organism_id].copy()\n    \n    print(f\"Found {len(organism_df):,} rows for {organism_name}\")\n    print(f\"Unique proteins: {organism_df['string_external_id'].nunique():,}\")\n    print(f\"Unique tissues: {organism_df['sample_organ'].nunique()}\")\n    \n    return organism_df\n\n# Filter for target organism\norganism_df = filter_organism_data(df, TARGET_ORGANISM)\norganism_df.head()"
  },
  {
   "cell_type": "markdown",
   "id": "de5f725a",
   "metadata": {},
   "source": "# Load Protein Embeddings\n\nLoad both PPI network embeddings and sequence embeddings from STRING database."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a545c0",
   "metadata": {},
   "outputs": [],
   "source": "def load_embeddings(organism_key, embedding_type='network'):\n    \"\"\"\n    Load protein embeddings from HDF5 files.\n    \n    Args:\n        organism_key: 'human' or 'mouse'\n        embedding_type: 'network' or 'sequence'\n    \n    Returns:\n        tuple: (embeddings_array, protein_list, metadata_dict)\n    \"\"\"\n    organism_id = ORGANISM_CONFIG[organism_key]['id']\n    \n    # Construct filename\n    filename = project_root / f\"{organism_id}.protein.{embedding_type}.embeddings.v12.0.h5\"\n    \n    if not filename.exists():\n        raise FileNotFoundError(f\"Embedding file not found: {filename}\")\n    \n    print(f\"Loading {embedding_type} embeddings from: {filename.name}\")\n    \n    try:\n        with h5py.File(filename, 'r') as f:\n            # Load metadata\n            metadata = {}\n            for key in f['metadata'].attrs.keys():\n                metadata[key] = f['metadata'].attrs[key]\n                print(f\"{key}: {metadata[key]}\")\n            \n            # Load embeddings and protein names\n            embeddings = f['embeddings'][:]\n            proteins = [p.decode('utf-8') for p in f['proteins'][:]]\n            \n        print(f\"Successfully loaded {len(proteins):,} {embedding_type} embeddings\")\n        return embeddings, proteins, metadata\n        \n    except Exception as e:\n        print(f\"Error loading {embedding_type} embeddings: {e}\")\n        raise\n\n# Load PPI network embeddings\nppi_embeddings, ppi_proteins, ppi_metadata = load_embeddings(TARGET_ORGANISM, 'network')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f94c5",
   "metadata": {},
   "outputs": [],
   "source": "# Load sequence embeddings\nsequence_embeddings, sequence_proteins, sequence_metadata = load_embeddings(TARGET_ORGANISM, 'sequence')\n\n# Verify protein lists match between embedding types\nproteins_match = set(ppi_proteins) == set(sequence_proteins)\nprint(f\"\\nProtein lists match between PPI and sequence embeddings: {proteins_match}\")\n\nif not proteins_match:\n    ppi_set = set(ppi_proteins)\n    seq_set = set(sequence_proteins)\n    print(f\"PPI only: {len(ppi_set - seq_set)} proteins\")\n    print(f\"Sequence only: {len(seq_set - ppi_set)} proteins\")\n    print(f\"Common proteins: {len(ppi_set & seq_set)} proteins\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4229b",
   "metadata": {},
   "outputs": [],
   "source": "def create_embedding_dataframes(embeddings, proteins, embedding_type):\n    \"\"\"Convert embeddings to DataFrame format.\"\"\"\n    return pd.DataFrame({\n        'string_external_id': proteins,\n        f'{embedding_type}_embeddings': list(embeddings)\n    })\n\ndef merge_with_embeddings(abundance_df, ppi_embeddings, ppi_proteins, seq_embeddings, seq_proteins):\n    \"\"\"\n    Merge abundance data with both PPI and sequence embeddings.\n    \n    Returns:\n        pd.DataFrame: Merged dataframe with embeddings\n        dict: Merge statistics\n    \"\"\"\n    print(\"Creating embedding dataframes...\")\n    \n    # Create embedding dataframes\n    ppi_df = create_embedding_dataframes(ppi_embeddings, ppi_proteins, 'PPI')\n    seq_df = create_embedding_dataframes(seq_embeddings, seq_proteins, 'sequence')\n    \n    print(\"Merging abundance data with embeddings...\")\n    \n    # Merge with PPI embeddings\n    merged_ppi = abundance_df.merge(ppi_df, on='string_external_id', how='inner')\n    print(f\"After PPI merge: {len(merged_ppi):,} rows ({len(merged_ppi)/len(abundance_df)*100:.1f}% of original)\")\n    \n    # Merge with sequence embeddings\n    final_df = merged_ppi.merge(seq_df, on='string_external_id', how='inner')\n    print(f\"After sequence merge: {len(final_df):,} rows ({len(final_df)/len(abundance_df)*100:.1f}% of original)\")\n    \n    # Statistics\n    stats = {\n        'original_rows': len(abundance_df),\n        'original_proteins': abundance_df['string_external_id'].nunique(),\n        'final_rows': len(final_df),\n        'final_proteins': final_df['string_external_id'].nunique(),\n        'merge_success_rate': len(final_df) / len(abundance_df),\n        'protein_coverage': final_df['string_external_id'].nunique() / abundance_df['string_external_id'].nunique()\n    }\n    \n    return final_df, stats\n\n# Perform the merge\nfinal_merged_df, merge_stats = merge_with_embeddings(\n    organism_df, ppi_embeddings, ppi_proteins, sequence_embeddings, sequence_proteins\n)\n\n# Display merge statistics\nprint(f\"\\n=== Merge Statistics ===\")\nprint(f\"Original proteins: {merge_stats['original_proteins']:,}\")\nprint(f\"Final proteins: {merge_stats['final_proteins']:,}\")\nprint(f\"Protein coverage: {merge_stats['protein_coverage']:.1%}\")\nprint(f\"Row merge success: {merge_stats['merge_success_rate']:.1%}\")\n\nfinal_merged_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee6378",
   "metadata": {},
   "outputs": [],
   "source": "# Data Quality Assessment\nprint(\"=== Data Quality Summary ===\")\nprint(f\"Final dataset shape: {final_merged_df.shape}\")\nprint(f\"Memory usage: {final_merged_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n\n# Check for missing values\nmissing_cols = final_merged_df.isnull().sum()\nmissing_cols = missing_cols[missing_cols > 0]\nif len(missing_cols) > 0:\n    print(f\"\\nColumns with missing values:\")\n    for col, count in missing_cols.items():\n        print(f\"  {col}: {count:,} ({count/len(final_merged_df)*100:.1f}%)\")\nelse:\n    print(\"\\nNo missing values found in merged dataset\")\n\n# Display sample of the data\nprint(f\"\\nSample data preview:\")\nfinal_merged_df.sample(3)[['organism_name', 'sample_organ', 'abundance', 'string_external_id']]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe9019",
   "metadata": {},
   "outputs": [],
   "source": "def save_processed_data(df, organism_key):\n    \"\"\"Save the processed merged dataframe.\"\"\"\n    organism_name = ORGANISM_CONFIG[organism_key]['name'].replace('.', '_').lower()\n    output_path = project_root / f\"{organism_name}_abundance_PPI_seq_embeddings.parquet\"\n    \n    print(f\"Saving processed data to: {output_path}\")\n    \n    # Optimize memory before saving\n    print(f\"Pre-save memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n    \n    # Save with compression\n    df.to_parquet(output_path, engine='pyarrow', compression='snappy')\n    \n    file_size = output_path.stat().st_size / 1024**2\n    print(f\"Saved successfully! File size: {file_size:.1f} MB\")\n    \n    return output_path\n\n# Clean up large variables before saving to free memory\ndel ppi_embeddings, sequence_embeddings\ndel ppi_proteins, sequence_proteins\n\n# Save the processed data\noutput_file = save_processed_data(final_merged_df, TARGET_ORGANISM)\n\nprint(f\"\\n=== Processing Complete ===\")\nprint(f\"Target organism: {ORGANISM_CONFIG[TARGET_ORGANISM]['name']}\")\nprint(f\"Final dataset: {final_merged_df.shape[0]:,} rows, {final_merged_df.shape[1]} columns\")\nprint(f\"Output file: {output_file.name}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PMDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}